{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c35d49e-b597-4716-b03d-9cf83944e1c6",
   "metadata": {},
   "source": [
    "# __Ques 1__\n",
    "1. __Overfitting:__An overfitting model in ML means the ML model with high bias and low variance in this the test set are low variable i.e., This is because the model has learned to fit the noise in the training data rather than the underlying patterns or relationships\n",
    "2. __Underfitting:__ An underfitting model in Ml model means the Ml model with high vatriability and high bias. In this model the test set are high variable i.e., low in accuracy and the train set are high bias i.e., low in accuracy.  poor in accuracy and the Train set are high bias i.e., High in accuracy.This occurrs when the train set is too simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd16d0-d78f-4a20-afda-741256f7ca04",
   "metadata": {},
   "source": [
    "# __Ques 2__\n",
    "Overfitting is a common problem in machine learning where a model is too complex and has learned the noise in the training data rather than the underlying patterns or relationships. This results in poor performance on new, unseen data.\n",
    "<br><br>\n",
    "To reduce overfitting, there are several techniques that can be used:\n",
    "\n",
    "1. __Cross-validation:__ Cross-validation involves splitting the data into training and validation sets, and then training the model on the training set and evaluating its performance on the validation set. This allows us to check whether the model is overfitting or underfitting the data.\n",
    "\n",
    "1. __Regularization:__ Regularization is a technique that adds a penalty term to the loss function to discourage the model from fitting the noise in the data. Common regularization techniques include L1 regularization (which encourages sparse solutions) and L2 regularization (which encourages small weights).\n",
    "\n",
    "1. __Early stopping:__ Early stopping involves stopping the training process before the model has fully converged, based on the performance on the validation set. This prevents the model from overfitting the training data by stopping the training process before the model starts to overfit.\n",
    "\n",
    "1. __Dropout:__ Dropout is a regularization technique that randomly drops out some of the nodes in a neural network during training. This forces the network to learn more robust features that are not dependent on any single node.\n",
    "\n",
    "1. __Data augmentation:__ Data augmentation involves generating new data by applying transformations such as rotation, scaling, and flipping to the existing data. This increases the amount of training data and reduces the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c961ad-5985-4bc1-8b47-9c28be3581e7",
   "metadata": {},
   "source": [
    "# __Ques 3__\n",
    "Under fitting occurs when the data perform poorly on the training and test data this is because the model is to simpleor is nor trained enough resulting in the lack of traing of the model hence the model fails for both train set and test set\n",
    "<br><br>\n",
    "Secnarios where underfittign can occur in ML:-\n",
    "1. __Not Trained Enough:__ If the model is not trained enough than it is not able to find the underlying patterns in the data set resulting in poor performanse in both train and test set.\n",
    "2. __Oversimpled Train set:__ If the train set is too simpel then the model did'nt able to get fimilar to complex sets resulting in the poor performance and low accuracy on test and train set\n",
    "3. __Wrong Feature selection:__ It can also be a reason of the failing of the model on train and new data test as it dure to wrong feature selection. The model will not be able to train itself properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489c885-3fc7-4896-8217-0c23be826908",
   "metadata": {},
   "source": [
    "# __Ques 4__\n",
    "__Bias_variance Trade off:__<br>\n",
    "Bias-variance trade off is the concept of ML modelling that shows the inter relationship between bias(fitting the training data) and variance(genralizing the new data).<br>\n",
    "<br>\n",
    "__Diffrence Between Bias and Model:__<br>\n",
    "1. __Bias:__ It Tells the fitting of the training data. If a ML model is too simple or if the training set is not have proper featrues or if the model is not trained enough than any of thse can give birth the underfittign of teh model in whuch the trainig set be too simple(genrally) the model was not able to find the complex between it\n",
    "2. __Variance:__ I tells the genralizing of teh new data. If a ML model is too complex and have noices than it did't learn properly how to analyze the data hence resulting in a case where teh model works perfectly for traning data but fails on new data and this problem is called overfittign and  occur in High Variance condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f728ae-0759-438c-bfaa-edb6a2a44390",
   "metadata": {},
   "source": [
    "# __Ques 5__\n",
    "__Train/Test split:__ Split the dataset into two parts, a training set, and a test set. Train the model on the training set and evaluate its performance on the test set. If the model performs well on the training set but poorly on the test set, it may be overfitting. If the model performs poorly on both the training and test sets, it may be underfitting.<br>\n",
    "<br>\n",
    "__Cross-validation:__ Divide the dataset into k folds, train the model k times on k-1 folds and evaluate its performance on the remaining fold. Repeat this process for each fold and compute the average performance. If the model performs well on the training data but poorly on the validation data, it may be overfitting. If the model performs poorly on both the training and validation data, it may be underfitting.<br>\n",
    "<br>\n",
    "__Learning curves:__ Plot the performance of the model on the training and test data as a function of the number of training examples. If the model has high variance, the training error will be low, and the test error will be high, indicating overfitting. If the model has high bias, both the training and test errors will be high, indicating underfitting.<br>\n",
    "<br>\n",
    "__Regularization:__ Introduce regularization terms such as L1 or L2 regularization to the loss function. If the regularization parameter is too high, the model may underfit. If it's too low, the model may overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2ba25-a089-444b-81a0-471417c3800b",
   "metadata": {},
   "source": [
    "# __Ques 6__\n",
    "1. __Bias:__ It Tells the fitting of the training data. If a ML model is too simple or if the training set is not have proper featrues or if the model is not trained enough than any of thse can give birth the underfittign of teh model in whuch the trainig set be too simple(genrally) the model was not able to find the complex between it\n",
    "2. __Variance:__ I tells the genralizing of teh new data. If a ML model is too complex and have noices than it did't learn properly how to analyze the data hence resulting in a case where teh model works perfectly for traning data but fails on new data and this problem is called overfittign and  occur in High Variance condition.\n",
    "\n",
    "__High Bais Model:__ A high bias model can led to underfitting of the data. In this the model does not get traineed properly resulting in low accuracy in traning and also low accuray in new data set.\n",
    "<br><br>\n",
    "__High Variance Model:__ A high variace model can led to overfitting or underfitting if the bias is also high than teh variance surely will be high hence its underfittign and is the bais is low and only variance is high than its overfitting. the overfittign occurs because the traning data set is too complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176813ec-d6df-4a7b-be62-4ebed9e1fa9a",
   "metadata": {},
   "source": [
    "# __Ques 7__\n",
    "Regularization in machine learning is a technique used to prevent overfitting by adding a penalty term to the loss function. The penalty term discourages the model from fitting the training data too closely, leading to a more generalized model that can perform better on new, unseen data.<br><br>\n",
    "There are two main types of regularization techniques:\n",
    "\n",
    "1. __L1 regularization__, also known as Lasso regularization, adds the sum of the absolute values of the model's weights to the loss function. This penalty term shrinks the weights of the less important features to zero, effectively performing feature selection and reducing the complexity of the model. L1 regularization is particularly useful when there are many irrelevant or redundant features in the data.\n",
    "\n",
    "1. __L2 regularization__, also known as Ridge regularization, adds the sum of the squares of the model's weights to the loss function. This penalty term penalizes large weights and encourages the model to use smaller weights, resulting in a smoother and more generalized model. L2 regularization is particularly useful when all the features are important and contribute to the model's performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
